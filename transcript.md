Summarize the following transcript into bullet points: /home/kdlocpanda/projects/personal/my_repos/youtube_summarizer/How to Make AI VIDEOS (with AnimateDiff Stable Diffusion ComfyUI Deepfakes Runway).mp4:
 AI videos is one of the hottest trends in tech. Deepfakes animated videos, video to video generation techs to video. Today we're going to go over a primer on how you two can get familiar with the latest technologies here. And how you can make your own video similar to if you saw my last video, the AI short on how AI takes over the world check it out if you haven't. There's a lot of AI art in there and I'll show you how I was able to make some of those today. Shall we just jump right in? Now there's an easy way and a hard way to do this. Both are interesting actually. The easy way is simply using a service like say runaway.mout.com which I'll show you towards the end of the video but both are based on using stable diffusion which is an open source project. And so the hard way is really just running your own stable diffusion instance on your own computer. Now since I'm using a Mac actually today I'll be using a hosted version of stable diffusion. There's a number of services that will not be using today as runaway diffusion.com. If you do have a Windows machine you'll be able to run this natively. If you have a Mac there are some new projects lately that's trying to build in better support. But essentially we'll be using animated stable diffusion with comfy UI to generate the AI videos. So animated. What is it? It's essentially framework for animating images. Then you have stable diffusion which is the text to image AI generator and then comfy UI which is a note based editor for this project. So to get started I'll be using runaway diffusion which is essentially stable diffusion in the cloud fully managed. The first thing we want to do here is we want to select a UI interface for stable diffusion because I think stable diffusion is just like a command line interface. So there's all these UI's for it. We'll be using comfy UI which is note based drag and drop. So if I select this I'm going to choose the medium hardware comfy UI. Cloud for version of using the beta release and then click launch. And so this is now starting up the machines. What we'll be doing today is video to video AI generation where we'll be modifying the style of an existing video. If you take a look at the sky that there's actually a video to video control net JSON file that you can download, there will be a link in the description below so you can follow along. And so we have your comfy UI with stable diffusion loaded. And I'm just going to click clear the workspace and drag in the JSON file. So this is comfy UI basically. And you can see there's all these workflows and processes that the images come in and out of and it just helps to refine the images and parameters. You can see for each of these nodes there's all these different parameters that you can set but essentially it's going to start with an input image here. And so we're going to be able to load a video or a set of images. I'm going to load in a video so I'm going to rewire the image for this node to the video node. And then here on my file manager I can upload the video video dot MP4. And so this is the image I've added into it is of me typing here looking sharp. And I'm going to reference that here in the path. So it's going to be slash mount slash private slash video dot MP4. Now if I click Q prompt I can see it's starting to bring in the video but there are some errors here highlighted in the red boxes and if I click the server manager I can actually take a look at the log here. And it's just not able to find some of these checkpoints. Now your checkpoint is essentially like a snapshot of a pre-trained model. So it's a way for you to style the type of images that you want. And so there's all sorts of different checkpoints here. There's like Disney Pixar cartoon style. So this is trained on like Disney Pixar cartoons. And you'll notice there's also S.D. XL models. The S.D. XL models is a different type of model. Explain that a little bit later but it's kind of like an incompatible style so you don't want to pick that up for this example. And then we're going to pick a VAE here. Now I don't know what the VAE is and this is part of the problem with this workflow. It is quite complicated. So we can see here it's also going to be generating this line model. So maybe doing some edge detection here for the images. And maybe it's using the line art to determine some of the motion. You can check where the inputs and outputs are flowing into here. It flows into some case and clear notes. And then there's a prompt here where you can change the subject matter. So we'll say it is a cyborg male robot typing. And so now if I click to prompt it's going to start generating this. And we can see what we get. Check the server console and I can see it's actually starting to generate the images here zero frames out of 25. We can take a look at the result. So apparently it had created this animated gift. Yeah, looks Pixar style. What you can do is actually come into this node and change it to MP4 file format and then re-cute that. And the cool thing about comfy UI is it's smart enough to know not to reprocess the whole entire workflow. But just that last node that had changed. So the process is faster this time. We can see the MP4 file here. This is the final AI video that it had generated for us. It looks all right. There's actually a nice website, a civid.com which has a bunch of pre-trained art styles that you can use to generate your own videos. And so for example, let's say you want an animation style. There's a model here known as dark sushi mix which was trained on these animation styles. And so you can just copy this URL and run the fusion actually has support for bringing in civid AI models. So all you have to do is click the civid AI button here search for that URL and then you can download that into your workspace. Now if I come in here to change this to dark sushi style, so I'll click that and then click Q prompt then it will run through the sole workflow and we'll see a different style of theization for this in anime style. Now the other way to do this is the arguably easier way if you don't want to run your own node is to simply use runawayML.com which is essentially a hosted version of stable diffusion. I think they even helped develop stable diffusion but they do have something new here Gen2 which is generating video using text images or both. And so typically what I would do is I would first bring in an AI generated image. I generated the images using mid journey but you can also use runaway or dolly or any number of AI image generators. If you're using mid journey you would come into the mid journey discord and then you would use the imagine command to generate an image like imagine a dog on a beach and further realistic style and then you can give it like an aspect ratio of 16 by 9 and it would generate this for you and it shows up in your mid journey feed. You can see all these other images I generated earlier. Download the image of your dog bring it into runaway and then give it some camera motion like you can zoom out you can roll you can pan and they even have something here known as motion brush where you can just select the area of the image that you want to animate and decide if you wanted to come closer further left or right save that and then you can click generate. And the nice thing about runaway is it's fairly fast after two minutes or so we have a dog here just smiling on the beach with some motion and you can even extend this by another four seconds if you like you can also give a description here to actually describe how you want the scene to animate and it's going to take this into account here and so that's another way to create AI videos and it's a good way for animating photographs or even memes. Now aside from runaway gen 2 they also have gen 1 which is the video to video generation which is similar to the animate differ example we were doing earlier so for this I can also bring in the same video ahead earlier of myself typing give a style reference or even a prompt so we'll say like a sideboard machine robot and then you can generate this runaway actually has preview styles as well which is a great artist tool. I think increasingly the ease of use and the UI is important for these AI tools especially for more creative types but we can see here a quick preview of the styles and then generate the video for the one that we want here and so this here's the video generated with runaway ML a simpler process but perhaps less customizable than running your own nodes and then a few other interesting tools here if you're looking to create deep fake videos for example that we have achieved aGI then one thing I found useful is WAF to lip where essentially you can just upload a video and a voice sample and it's going to sink the lips to the video so it's pretty much plug and play there's even this GitHub project that just found SD WAF to lip UHQ which apparently is an improvement on the original using some stable diffusion techniques but at least the version I used was the original just a WAF to lip tool and they have this website sinklabs.so I like these hosted versions just because I don't have the time necessarily to be playing with all these tools where you can just upload a video and an audio file click up on it and it just kind of trains this mostly and generates something for you for the audio track if you're looking to clone a voice and to have it say something what I would use is this tool on replicate.com now replicate is essentially hosted machine learning models and so here I've loaded the Tordoist TDS model you can just search for it generate speech from text clone voices from MP3 files type in the text that you wanted to say upload a sample of the voice and then click run and it generates the audio file essentially if you're having trouble with that you can also try 11 laps the IO which also does generate the voice AI and they have this as a service and then finally before I close I thought that would show you the latest in stable diffusion stable diffusion Excel turbo which is essentially real time image generation so you know the original model was stable diffusion after that they came out with stable diffusion Excel which is an improvement on the model with better human anatomy and just more accurate image generation and so then just recently they came out with S.D. Excel turbo which is real time text image generation they have a sample website here clip drop.co where you can play with this yourself and so I can say here dog on a beach and it's going to be generating this very quickly and like maybe I can change this to a cat playing with a ball right so it's just generating this very quickly and if you're curious you can also run these workflows yourself if you come to the confi ygithub you can see the examples and so they have sdxl turbo here where you can actually download this workflow using confi y so you just download that bring in the sdxl turbo checkpoint and then click q prompt and this is going to run through this much faster so there have it and the image of a fox in the bottle but I can also change this to say a cat in the bottle and it's going to regenerate very quickly using the sdxl turbo model and so there have a basic primer on AI video and AI art generation personally for me one of my favorite and maybe the easiest tool to get started with this is runawayeml.com where they have like text video generation video to video all these different tools image to image generation expand the image subtitles and so forth but let me know any other interesting tools in the comments below or any questions you might have hope you enjoyed the video see you in the next one thanks bye